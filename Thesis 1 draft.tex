\documentclass[conference]{IEEEtran}
\usepackage{amsmath}
\interdisplaylinepenalty=2500
\usepackage{array}
\usepackage{url}
\usepackage{graphicx}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\usepackage{float}
\usepackage{placeins}
\usepackage{listings}

\begin{document}
\title{A Data Mining Framework for Analysing Geospatial-Temporal Data}
\author{\IEEEauthorblockN{Raunak Sarbajna}
\IEEEauthorblockA{Department of Computer Science\\
Lamar University\\
Beaumont, Texas, USA\\
Email: rsarbajna@lamar.edu}
\and
\IEEEauthorblockN{Sujing Wang}
\IEEEauthorblockA{Department of Computer Science\\
Lamar University\\
Beaumont, Texas, USA\\
Email: sujing.wang@lamar.edu}}
%\author{\IEEEauthorblockN{Raunak Sarbajna\IEEEauthorrefmark{1},
%Sujing Wang\IEEEauthorrefmark{2}}
%\IEEEauthorblockA{Department of Computer Science,
%Lamar University\\
%Beaumont\\
%Email: \IEEEauthorrefmark{1}rsarbajna@lamar.edu,
%\IEEEauthorrefmark{2}swang3@lamar.edu}}
\maketitle

\begin{abstract}
In this paper, we introduce a new data mining framework for analysing spatio-temporal data. It introduces polygon based change analysis techniques and automatic storytelling methodology for spatio-temporal data. Change analysis and automatic storytelling are essential techniques in understanding larger patterns and trends in multifaceted, time-series geographic data. We evaluate the effectiveness of our framework through case studies involving Twitter emotion  data and North American Drought data. The experimental results show that our framework can discover interesting patterns and useful information from spatial-temporal data.
\end{abstract}

\begin{IEEEkeywords}
change analysis, storytelling, sentiment analysis, polygon, spatio-temporal data.
\end{IEEEkeywords}


\section{Introduction}
% no \IEEEPARstart
Analyzing change in spatial data is critical for many applications including developing early warning systems that monitor environmental conditions, detecting political unrest and crime monitoring.
 
Change analysis models are essential in understanding larger patterns and trends in multifaceted, time-series geographic data. The purpose of this study is to detect spatiotemporal changes in land use within sequential (time-series) maps. Changes in land use can be categorized by the complex interaction of structural and behavioural factors associated with technological capacity, demand, and social relations that affect both environmental capacity and the demand, along with the nature of the environment of interest.

Modern technology digitizes wide sources of information constantly, with hour after hour of data being stored and most of it being unprocessed raw information. This is especially true with remote sensing networks and resultant geo-spatial imagery, coming from satellites, drone captures and flyover shots. Processed, georeferenced datasets from sources as diverse as gravity measurement a la GRACE [23] to ocean circulation mapping [24]. Much of this data has meta-information associated with it which can reveal patterns or trend if properly classified and analysed. Performing basic change analysis on a subset of this data gives us a lot of insights that would otherwise go unrecognised.

A common understanding is that most big data available today is either archival, media or web scrapes [25]. However, a large source of that data is actually from Geographic Information Systems (GIS), and the tools available to interpret them easily are lacking [26]. The multiplicity of APIs have standardized access and structuring, but they limit much of the meta-data associated with them. Most publicly available (i.e. non-governmental) 'big data' sources with spatial components revolve around data scraped from mobile software platforms, including twitter, Instagram, Snapchat and reviews on mapping apps [26]. An example of such insights include the Livehoods project, conducted by the Carnegie Mellon University which proposed that "the character of an urban area is defined not just by the the types of places found there, but also by the people who make the area part of their daily routine." [27] However, an unfortunate side effect of such large datasets generated and controlled by privately held corporations do raise concerns about gated access and limitations in publications. [26] The recent API changes by Google Maps, for instance, aptly show that data content, and thus its meaning, is subject to regulation that is outside the control of researchers.
 
The goal of this research project is to detect and analyse how the patterns of features change over time and space in spatiotemporal land use datasets, while being completely data agnostic in form and function. All polygons under consideration are closed spatial  georeferenced sets, rather than raster imagery.

Our approach provides a change monitoring framework which creates a change graph that captures the changes in spatial land uses clusters and a change summarization framework that creates specific change summaries based on the change graph based on the change story types. 

Our research contributions are summarized as follows:

\begin{enumerate}
 \item A novel change analysis framework that works on vectored polygonal datasets
 \item New change predicates that are data agnostic and can work on a large spectrum of data
 \item A new measure of interestingness to aid in generating automated storytelling based the change analysis results
\end{enumerate}

The rest if the paper is structured as follows. Section 2 reviews previous literature on the subject and discusses related work.  Section 3 introduces our data mining framework and lays out the methodology in detail. Section 4 evaluates the framework with case studies on drought datasets pre and post Harvey, pre and post California wildfires and twitter emotion polygons. Section 5 provides a conclusion and discusses potential future expansions to the framework.


\section{Related Work}

A survey of the classical change detection algorithms can be found in the Lu et al. [3] paper and tells us that the integrated GIS and remote sensing approaches yield the best results. However, they are very sensitive to registration accuracies between images. Thus, images must be properly orthorectified and georeferenced, especially because the changes in the emotion polygons are so subtle. This assumes the emotions are to be treated as just another feature in the map, like any other category.

Since our data is primarily in an urban environment, with all the grid like rigidity that entails, it is a good idea to look at change detection algorithms optimized for urban environments. One of the hardest aspects to measure is to distinguish between change and no-change, as well as different kinds of change. Comparing image differencing, image regression, tasselled-cap transformation and chi square transformation, Ridd and Liu [3] find image differencing to be the most consistent, with a sustained overall accuracy of 
$>$80\%.

It is useful to have a programming-oriented study comparing several of the change detection algorithms using MATLAB, rather than pure application-oriented comparison, in order to have a benchmark. Minu and Shetty [5] analyzed image differencing, image ratioing, change vector analysis, tasseled cap transformation and principal component analysis for efficiency and effectiveness. Although their area of study was not urban but a variety of land use/ land cover, change vector analysis gave the best overall accuracy.
We also studied two novel methods that are recent developments and are showing promising results: Neighborhood Correlation Image and Comprehensive Change Detection Method, both of which are optimized for remote sensing imagery but can be adapted to vectorized maps without loss of generality.

The change detection model using Neighborhood Correlation Image (NCI) logic works because of the obvious fact that the same geographic area (e.g., a 3x3 pixel window) on two dates of imagery will tend to be highly correlated if little change has occurred, and uncorrelated when change occurs [1]. Computing the piecewise correlation between two data sets demonstrates that NCIs contain change information and that NCIs may be powerful tools for change detection.

A high-performance remote sensing method called Comprehensive Change Detection Method (CCDM) integrates spectral-based change detection algorithms and a novel change model called Zone, which extracts change information from two Landsat image pairs [2]. This can be easily modified to work on the Twitter-based emotional grading maps. This method is simple, easy to operate, widely applicable, and capable of capturing anthropogenic changes like our area of interest.

Storytelling techniques are effective summarization method to succinctly organize extensive information. Traditional storytelling has been mostly successful on news articles, blogs, as well as structured databases. However, traditional storytelling techniques tend to perform poorly on social media content, such as Twitter, where text lacks proper form and function [11]. Moreover, the ability to support dynamic storylines as they evolve is critical to modelling fast moving social media streams such as Twitter. Dos Santos et al. [21] introduced a set of methods to automatically derive stories over linked entities in tweets. They model a story as a graph of entities propagating through spatial regions in a temporal sequence, and controls search space complexity by suggesting regions of exploration. They developed algorithms to conduct storytelling to model tweets over space and time, reasoning over spatio-temporal features, and devise spatio-temporal storylines based on connectivity strength.

Kumar et al. [14] proposed an efficient storytelling implementation that embeds the CARTwheels [15] redescription mining algorithm which utilizes induced classification trees to model redescriptions in an A* search procedure, using the CARTwheels to supply next move operators on search branches to the A* search procedure. Vocht et al. [15] proposed the implementation of an optimized algorithm controlling the pathfinding process to obtain more homogeneous search domain and retrieve more links between adjacent hops in each path to improve the semantic relatedness of concepts mentioned in a story by increasing the relevance of links between nodes through additional domain delineation and refinement steps. Chen et al. [20] proposed a multimodal imitation learning via generative adversarial networks (MIL-GAN) method to directly model users' interests as reflected by various data by imitating users' demonstrated storylines. MIL-GAN model is designed to learn the reward patterns given user-provided storylines and then applies the learned policy to unseen data. Santos et al. [21] combined storytelling and Spatio-logical Inference (SLI) to generate rules of interaction among entities and measure how well they forecast a real-world event. 



Hossain et al [13] introduced Google Fusion Tables(GFT) that offers collaborative data management in the cloud for data scientists to enable the integration of increasingly complex geospatial data to support storytelling. The paper focused on introduction of overview of map processing in GFT, the architecture overview of GFT, and how to scale to large datasets, massive and complex polygon datasets. GFT provides a useful tool for storytelling through interactive maps. 



Kumar et al. [14] formulated storytelling as a generalization of redescription mining. Stories are defined as chains of redescriptions. They proposed an efficient storytelling algorithm as A* search around the outputs of a CARTwhells redescription mining algorithm. The efficiency and scalability of the proposed algorithm were evaluated by three application case studies: word overlaps in large English dictionaries, exploring connections between gene sets in a bioinformatics data set, and relating publications in the PubMed index of abstracts. 



Hossain et al. [19] proposed an approach to automatically construct stories between entities in large document collections that can help from directed chains of relationships, with support for co-referencing, evidence marshaling, and imposing syntactic constrains on the story generation process. A new optimization techniques based on concept lattice mining is used to rapidly construct stories on massive datasets. 



Chen et al. [20] introduced an approach, multimodal imitation learning via generative adversarial networks (MIL-GAN) for generating storyline on unseen data. It can directly model users’ interests as reflected by various data. This approach is used to learn the reward patterns given user-provided storylines and then applies the learned policy to unseen data.   




Santos et al. [21] introduced three methods of association analysis, Distance-based Byesian Inference, Spatial Association Index, and Spatio-logical inference, to capture relatedness among real-world events in high data volumes, and to model similar events that are described disparately under high data variability. It takes as input a set of geotemporally-encoded text streams about violent events called “storylines”. This study demonstrated that spatio-temporal storytelling is able to capture important associations among violent events reported in social media and traditional datasets. 


\section{Methodology}


\subsection{Change Predicates}

We begin by creating our change predicates, based on the Aconcagua Framework [6]:

\begin{enumerate}
	\item $S-Continuing (c,m)  \leftrightarrow  Agreement (c,m) \geq 0.8$
	\item $B-Continuing(c,b)  \leftrightarrow  Oap (c,b) \geq 0.8$
	\item $Growing(c,m)  \leftrightarrow  Contaiverlnment (c,m) \geq 0.9 $
	\item $Shrinking(c,m) \leftrightarrow Growing (m,c)$
	\item $Disappearing(c) \leftrightarrow \exists i (belong-to(c,i) $
	\item $Novel (c) \leftrightarrow  \exists i (belong-to(c,i) and (i=1 or not(B-Continuing(c,i-1)) $
	\item $Shifting$

\end{enumerate}

Which are defined as:

\begin{itemize}
	\item $Agreement (c, m) = (area(c\cap m))/(area(c\cup m))$

	\item $Overlap(s,f)= area(p\cap (p_1\cap …\cap p_m))/area(p)$

\end{itemize}

\subsection{Point Map Data Sources}
We are implementing and improving upon the change analysis framework called Aconcagua [6]. The system expects an input of emotion polygons annotated with emotion assessment scores, with +1 representing a very high positive emotion and -1 representing a very high negative emotion. While this method does lead to inconsistencies in locations due to georeferencing inaccuracies, we find that the 8 ~ 10m precision [8] works well within city limits.

There are several ways of using the numerous point data we have obtained from this step into an actual polygonal map: 

\begin{enumerate}

\item Creating closed contour lines for contour lines that lie on the boundary of the observation area.
\item Creating a convex hull from points with similar scores.
\end{enumerate}

We elaborate on creating convex hulls in the following section.

\subsection{Polygonal Map Data Sources}


Our approach was using three primary set operations: union, intersection and erase. We calculate area of each individual polygon within each map layer. We then execute a union operation and calculate area. The union layer now contains the original areas of both layers and the areas of the overlapping polygons - we now need to query them properly to prepare for calculating the change percentage and tabulating intersection.

To outline the polygon, we examine several different methods:
\begin{enumerate}
	\item We find features common to either of the layers but not both, essentially performing a symmetrical difference
	\item We erase the larger of the polygons from the smaller, thus retaining only the growth, and do vice-verse for shrinking
	\item We perform simple intersection and then invert selection to get changed regions. 
\end{enumerate}

Our approach then combined several techniques:
\begin{enumerate}
	\item \textbf{Data Pre Processing} This involves curation of datasets with obvious georeferencing errors. This would preferable be done by minimizing the root mean square error. We initially.
	\item \textbf{Parametrization of polygons} Calculate shape and area parameters for each individual polygon with each map layer.s
	\item \textbf{Analysis through Symmetrical Difference} Extract features common to either of the layers
	\item \textbf{Polygon Union Computation} Union sequential layers to contain the original areas of both layers and the areas of overlapping polygons
	\item \textbf{Polygon Erase Operation} Erase larger polygons from smaller (or vice-versa) for detection of growth/shrinking
	\item \textbf{Polygon Intersection/Invert} Selecting and then labelling the changed regions
\end{enumerate}

The framework can be found in Figure \ref{Figure 1}.

\begin{figure}[ht]
\centerline{\includegraphics[width=\linewidth]{"Figure 14".png}}
\caption{The Framework Architecture}
\label{Figure 1}
\end{figure}
%This procedure is outlined in Figure \ref{Figure 5}
%
%\begin{figure}[ht]
%\centerline{\includegraphics[width=\linewidth]{"Figure 5".png}}
%\caption{Polygons generated from Sentiment analysis}
%\label{Figure 5}
%\end{figure}



\subsection{Change Storytelling}

In order to able to tell a coherent spatio-temporal data story from the change analysis output, we need to be able to pick out resultant change polygons that have seem to have significant impact on the dataset as a whole. We propose using an interestingness function to look for these.[11]

We choose a set of change polygons SCP. These not only contains those objects but also their associated characteristics. For example, a SCP could contain a set of spatial clusters with polygon, their average drought score, total area, centroid coordinates and other summaries for each spatial cluster. We define the function as:
\[f:SCP\ \rightarrow\ \left[0\right.,\left.\infty\right)\]

We define a threshold $\omega$, which ensures that a narrative will only be generated an an object $p \in SCP$ such that $f(p) \ge \omega$. Example parameters for $\omega$ include  

\begin{itemize}
\item $Max\left(\frac{area\left(Polygon\ P_i\right)}{\sum_{1}^{i}area\left(Polygon\ P_i\right)}\right)$
\item $Max\left(Percentage\ Change\ in\ Polygon\right)$
\item Largest shift in polygon centroids
\end{itemize}

The threshold parameters need to be finely tuned so as to not exclude those polygons who fall through exceptions. Once we have a suitable selection of polygons and have chose a threshold value, we can create a summary narrative. 


\section{Case Study}

\subsection{Preprocessing Data Sources}

Our original data source for the drought sets were defined by the USDA as seen in Figure \ref{Figure 2}.

\begin{figure}[ht]
\centerline{\includegraphics[width=\linewidth]{"Figure 6".png}}
\caption{United Sates Drought Monitor Data}
\label{Figure 2}
\end{figure}

Our initial approach to this problem was to store all shapefiles in a postgres database with a GIS addon and perform operations in python. We used psycopg2 and osgeo libraries to import, process and visualize maps. However, this lead to many problems with interconversions between georeferencing schemes, while converting from WKT geometry to PostGIS geography.

We start with basic point data, which contains latitude/longitude, along with metadata identifying value of interest, whether that is drought level or emotion value. We insert the contents of the shapefile into a PostGreSQL database using the shp2pgsql toolkit that comes along with the PostGIS extension. The results can be seen in Figure \ref{Figure 3}.

\begin{figure}[ht]
\centerline{\includegraphics[width=\linewidth]{"Figure 3".png}}
\caption{Twitter shapefile data inserted into Postgres db}
\label{Figure 3}
\end{figure}

As our framework relies on using polygonal data, we cannot use this. So we begin by creating a convex hull of points and inserting it into a new table, while preserving related meta-data. Due to engine limitations, we need to be careful and insert only those convex hulls that are polygons specifically, not points or lines.

\begin{enumerate}
	\item Our query for this purpose was:
\begin{lstlisting}
INSERT INTO public.june1poly (avgscor, 
numtwts, geodata, id, batchnm, geom)

(SELECT d.avgscor, d.numtwts, d.geodata, 
d.id, d.batchnm, ST_ConvexHull
(ST_Collect(d.geom))
FROM public."2014-06-01 " AS d
GROUP BY (d.id, d.avgscor, d.numtwts,
d.geodata, d.batchnm)

HAVING ST_GeometryType(ST_ConvexHull
(ST_Collect(d.geom))) = 'ST_Polygon')
\end{lstlisting}
	\item Then we insert the centroid of each polygon into the table using the query:

\begin{lstlisting}
UPDATE public.june1poly 
SET centroid=ST_Centroid(geom)
\end{lstlisting}

	\item We repeat this process for every shapefile needed.
\end{enumerate}

\subsection{Drought Data Sources}

There are two different datasets under consideration here. First is the Spatiotemporal Drought Datasets from the North American Portal: https://www1.ncdc.noaa.gov/pub/data/nidis/shapefiles/.



The spatial reference for the Drought shapefiles can be seen in figure 4.  

\begin{figure}[ht]
\centerline{\includegraphics[width=\linewidth]{"Figure 1".png}}
\caption{Spatial Reference for Drought data}
\label{Figure 4}
\end{figure}



The layer definition for the data sets can be seen in tables 1 and 2.

\begin{table}[]
\caption{Layer Specification for Drought data}
\label{Table 1}
\centering
\begin{tabular}{llll}
Name    & Type    & Width & Precision \\
long    & Real    & 24    & 15        \\
lat     & Real    & 24    & 15        \\
id      & Integer & 9     & 0         \\
dnstyTh & Real    & 24    & 15        \\
avgScor & Real    & 24    & 15        \\
numTwts & Integer & 9     & 0         \\
stdDev  & Real    & 24    & 15        \\
batchNm & Integer & 9     & 0         \\
geoData & String  & 80    & 0        
\end{tabular}
\end{table}

\begin{table}[]
\caption{Layer Specification for Twitter data}
\label{Table 2}
\centering
\begin{tabular}{llll}
Name        & Type    & Width & Precision \\
FIPS\_ADMIN & String  & 4     & 0         \\
GMI\_ADMIN  & String  & 7     & 0         \\
ADMIN\_NAME & String  & 42    & 0         \\
FIPS\_CNTRY & String  & 2     & 0         \\
GMI\_CNTRY  & String  & 3     & 0         \\
CNTRY\_NAME & String  & 40    & 0         \\
POP\_ADMIN  & Integer & 9     & 0         \\
TYPE\_ENG   & String  & 26    & 0         \\
TYPE\_LOC   & String  & 50    & 0         \\
SQKM        & Real    & 16    & 2         \\
SQMI        & Real    & 16    & 2         \\
COLOR\_MAP  & String  & 2     & 0        
\end{tabular}
\end{table}

We will demonstrate three of these predicates here.

\begin{enumerate}
	\item To detect polygons that are increasing in size, we check for similar IDs, intersection and then the rate of overlap. We initially check whether the polygons intersect at all before querying for amount of overlap. This leads to faster processing as it discards the many combinations where the polygons don't touch each other. Our query is structured as:
\begin{lstlisting}
SELECT DISTINCT j2.* 
FROM public.june1poly j1,
public.june2poly j2 
WHERE ST_INTERSECTS(j1.geom, j2.geom)
AND  
(ST_AREA(ST_INTERSECTION(j2.geom, j1))
/st_area(j2.geom)) > .85
\end{lstlisting}

	\item To detect polygons that are shrinking  in size, we check for similar IDs, and lower rates of overlap. This can be modified based on need.
\begin{lstlisting}
SELECT DISTINCT j2.* 
FROM public.june1poly j1,
public.june2poly j2 
WHERE ST_INTERSECTS(j1.geom, j2.geom)
AND  
(ST_AREA(ST_INTERSECTION(j2.geom, j1))
/st_area(j2.geom)) < .25
\end{lstlisting}	

	\item To detect polygons that have shifted we compare their centroids and check if they have moved over 75km. Remember these polygons are created through a convex hull of points, which cannot ensure the centroid will lie within the polygon itself. Which is why we are taking a sufficiently large bounding value for the polygon.
\begin{lstlisting}
SELECT ST_Distance_Spheroid
(j1.centroid, j2.centroid, 
'SPHEROID["WGS 84",6378137,298.25]'), 
 j1.id FROM public.june1poly j1, 
 public.june2poly j2 
WHERE j1.id = j2.id AND 
ST_Distance_Spheroid(j1.centroid, 
j2.centroid, 
'SPHEROID["WGS 84",6378137,298.25]')>75000;
\end{lstlisting}	

\end{enumerate}

A sample of what these generated maps look like can be found in Figure \ref{Figure 5}

\begin{figure}[ht]
\centerline{\includegraphics[width=\linewidth]{"Figure 4".png}}
\caption{Polygons generated from Sentiment analysis}
\label{Figure 5}
\end{figure}

\subsection{Emotion Spatial Clusters from Twitter }

The dataset was constructed using "Geotagged Twitter posts from the United States" [9] and the tool used for creating the emotion clusters was the K2 framework [7]. The raw dataset contains the timestamp, longitude, latitude and the text of each tweet, which are then processed and tokenized.

The spatial reference for the twitter data can be seen in figure 62.
\begin{figure}[ht]
\centerline{\includegraphics[width=\linewidth]{"Figure 2".png}}
\caption{Spatial Reference for Drought data}
\label{Figure 6}
\end{figure}

According to K2, the best package for the emotion score was the Valence Aware Dictionary and Sentiment Reasoner (VADER) system [10], whose analyser parses the tokenized text and checks within a lexicon for words with strong sentiment. The final score is created from the weighted average of all sentimental words and lies within the range [-1, 1], as is required for our Aconcagua implementation.



The original polygons for the drought datasets, before our analysis was started, can be found in Figure \ref{Figure 7}

\begin{figure}[ht]
\centerline{\includegraphics[width=\linewidth]{"Figure 7".png}}
\caption{Continental USA  Original Drought polygon Distribution for 2017}
\label{Figure 7}
\end{figure}

We focus on two specific regions to highlight. First, figure \ref{Figure 8} shows the regions where areas of drought grew in California following the wildfires in 2017. We noticed patterns of increasing drought surrounding the regions that were burnt down, with especially large period upstream from rivers. Our procedure worked well dealing with the large number of small polygons in the region that occur due to isolated wildfires. However, we had difficult dealing with convex polygons that intersected with each other multiple times.

\begin{figure}[ht]
\centerline{\includegraphics[width=\linewidth]{"Figure 8".png}}
\caption{Change in drought affected regions in California following 2017 wildfires.}
\label{Figure 8}
\end{figure}

Next, we inspect the region in Texas after Hurricane Harvey. Figure \ref{Figure 9} shows the regions where drought affected regions increased following the disaster and Figure \ref{Figure 10} shows the regions where they decreased.

We observe that drought affected regions decrease at a high rate around the South Eastern Texas and Louisiana region, which follows common logic. However, there is no clear relation between drought prone regions and river basins close to the coast. We believe that high amount industrial regions create a micro climate that affects the water content entering the soil.

According to our results, the drought prone regions increased substantially around the West Texas region both during and after Hurricane Harvey, with a larger increase as winter came around. However, this could be an artefact of the fact that the original dataset had very few polygons in the Texas area, which leads to broader conclusions. Our technique is still severely dependent on the resolution of the input imagery.

\begin{figure}[ht]
\centerline{\includegraphics[width=\linewidth]{"Figure 9".png}}
\caption{Decrease in Drought Affected Regions in Texas following Hurricane Harvey}
\label{Figure 9}
\end{figure}

\begin{figure}[ht]
\centerline{\includegraphics[width=\linewidth]{"Figure 10".png}}
\caption{Increase in Drought Affected Regions in Texas following Hurricane Harvey}
\label{Figure 10}
\end{figure}

The results from the analysing the twitter emotion maps can be seen in Figure \ref{Figure 11}.

\begin{figure}[ht]
\centerline{\includegraphics[width=\linewidth]{"Figure 11".png}}
\caption{Change in emotion Polygons in New York State}
\label{Figure 11}
\end{figure}

\section{Conclusion and Future Work}

Our experimental studies show that our change detection and analysis framework can successfully detect changes in both polygonal and point-set land use spatiotemporal datasets. We show that our change predicates are indeed data agnostic and can work on a variety of data sources, including both polygonal and point datasets. We also see that our framework is able to handle both geographically large and small map sources without any significant georeferencing distortions being introduced. We have also defined an interestingness framework that shows promise.
 
There is a lot work left in dealing with irregularly shaped convex polygons that appear in poorly georeferenced real world data. We believe this is because the sensitivity of the geographic operations in PostGIS or ArcGIS. We are currently working on extending our framework to deal with edge cases like those.

We are also looking towards consolidating and extending some of the change predicates, especially trying to tune them for fine changes.

We will work on the creating change summaries and stories, as detailed in Section IV,  from the data in future.

\section*{Acknowledgment}

This research is supported by Lamar University Research Enhancement Grant


\newpage
\begin{thebibliography}{12}

  
\bibitem {1} J. Im and J. Jensen. 2005. A change detection model based on neighborhood correlation image analysis and decision tree classification. Remote Sensing of Environment 99, 3 (2005), 326–340. DOI:http://dx.doi.org/10.1016/j.rse.2005.09.008 
\bibitem {2} Suming Jin, Limin Yang, Patrick Danielson, Collin Homer, Joyce Fry, and George Xian. 2013. A comprehensive change detection method for updating the National Land Cover Database to circa 2011. Remote Sensing of Environment 132 (May 2013), 159–175. DOI:http://dx.doi.org/10.1016/j.rse.2013.01.012 
\bibitem {3} D. Lu, P. Mausel, E. Brondízio, and E. Moran. 2004. Change detection techniques. International Journal of Remote Sensing 25, 12 (June 2004), 2365–2401. DOI:http://dx.doi.org/10.1080/0143116031000139863 
\bibitem {4} Merrill K. Ridd and Jiajun Liu. 1998. A Comparison of Four Algorithms for Change Detection in an Urban Environment. Remote Sensing of Environment 63, 2 (1998), 95–100. DOI:http://dx.doi.org/10.1016/s0034-4257(97)00112-0 
\bibitem {5} S. Minu and Amba Shetty. 2015. A Comparative Study of Image Change Detection Algorithms in MATLAB. Aquatic Procedia 4 (March 2015), 1366–1373. DOI:http://dx.doi.org/10.1016/j.aqpro.2015.02.177 
\bibitem {6} K. Elgarroussi, S. Wang, R. Banerjee, and C. F. Eick, “Aconcagua: A Novel Spatiotemporal Emotion Change Analysis Framework,” in Proceedings of the 2Nd ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery, New York, NY, USA, 2018, pp. 54–61.
\bibitem {7} R. Banerjee, K. Elgarroussi, S. Wang, A. Talari, Y. Zhang, and C. F. Eick, “K2: A Novel Data Analysis Framework to Understand US Emotions in Space and Time,” Int. J. Semantic Computing, vol. 13, no. 01, pp. 111–133, Mar. 2019.
\bibitem {8} Geomenke, “How Accurate is the GPS on my Smart Phone? (Part 2),” Community Health Maps, 07-Jul-2014. .
\bibitem {9} W. Z.-M.-L. I. for the S. S. Person, “Geotagged Twitter posts from the United States: A tweet collection to investigate representativeness,” Jan. 2016.
\bibitem {10} C. J. Hutto, VADER Sentiment Analysis. VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social .. 2019.
\bibitem {11} X. Huang, A. An, and N. Cercone, “Comparison of Interestingness Functions for Learning Web Usage Patterns,” in Proceedings of the Eleventh International Conference on Information and Knowledge Management, New York, NY, USA, 2002, pp. 617–620.
\bibitem {} W. Wang, Lu Chen, K. Thirunarayan, and Amit P. Sheth, Harnessing Twitter "Big Data" for Automatic Emotion Identifcation, in Proc. of the IEEE 2012 International Conference on Privacy, Security, Risk and Trust and 2012 International Conference on Social Computing.
\bibitem {} R. F. Dos Santos, S. Shah, F. Chen, A. Boedi-hardjo, P. Butler, C.T. Lu and N. Ramakrishnan, "Spatiotemporal storytelling on twitter", Virginia Tech Computer Science Technical Report, 2015, [online] Available: http://vtechworks.lib.vt.edu/handle/10919/24701.
[20] D. Kumar, N. Ramakrishnan, R. F. Helm and M. Potts, Algorithm for storytelling, in IEEE Transactions on Knowledge and Data Engineering, 2008, Volume 20 Issue
6, pp. 736-751.
\bibitem {} N. Ramakrishnan, D. Kumar, B. Mishra, M. Potts and R. R. Helm, Turning CARTwheels: an alternating algorithm for mining redescriptions, in Knowledge Discovery and Data Mining, 2004, pp. 266-275.
\bibitem {} L. D. Vocht, C. Beecks, R. Verborgh, T. Seidl, E. Mannens and Rik Van De Walle, Improving Semantic Relatedness in Paths for Storytelling with Linked Data on the Web, in Revised Selected Papers of the ESWC 2015 Satellite Events on The Semantic Web: ESWC 2015 Satellite Events, May 31-June 04, 2015.
\bibitem {} Z. Chen, X. Zhang, A. P. Boedihardjo, J. Dai, Chang-Tien Lu, Multimodal storytelling via generative adversarial imitation learning, in Proceedings of the 26th International Joint Conference on Artifcial Intelligence, August 19-25, 2017, Melbourne, Australia.
\bibitem {} R. D. Santos, S. Shah, F. Chen, A. Boedihardjo, Chang-Tien Lu, N. Ramakrishnan, Forecasting location-based events with spatio-temporal storytelling, in Proceedings of the 7th ACM SIGSPATIAL International Workshop on Location-Based Social Networks, pp. 13-22, November 04-04, 2014, Dallas/Fort Worth, Texas.
\bibitem {} Jayant Madhavan, Sreeram Balakrishnan, Kathryn Brisbin, Hector Gonzalez, Nitin Gupta,Alon Halevy, Karen Jacqmin-Adams, Heidi Lam, Anno Langen, Hongrae Lee,Rod McChesney, Rebecca Shapley, Warren Shen, “Big Data Storytelling through Interactive Maps”
\bibitem {} M. Shahriar Hossain1, Patrick Butler1, Arnold P. Boedihardjo2, Naren Ramakrishnan, Storytelling in Entity Networks to Support Intelligence Analysts, KDD '12 Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, Pages 1375-1383 ,Beijing, China , August 12 - 16, 2012
\bibitem {} Zhiqian Chen , Xuchao Zhang , Arnold P. Boedihardjo , Jing Dai , Chang-Tien Lu, Multimodal storytelling via generative adversarial imitation learning, Proceedings of the 26th International Joint Conference on Artificial Intelligence, August 19-25, 2017, Melbourne, Australia 
\bibitem {} Raimundo F. Dos Santos, Jr. , Arnold Boedihardjo , Sumit Shah , Feng Chen , Chang-Tien Lu , Naren Ramakrishnan, The big data of violent events: algorithms for association analysis using spatio-temporal storytelling, Geoinformatica, v.20 n.4, p.879-921, October 2016
\bibitem {} Deept Kumar, Naren Ramakrishnan, Richard F. Helm, and Malcolm Potts. 2008. Algorithms for Storytelling. IEEE TKDE20, 6 (2 2008), 736–751.DOI:http://dx.doi.org/10.1145/1188913.1188915
\bibitem {} “GRACE Tellus Data,” GRACE Tellus. [Online]. Available: https://grace.jpl.nasa.gov/data/get-data. [Accessed: 26-Jul-2019].
\bibitem {} “SARAL - eoPortal Directory - Satellite Missions.” [Online]. Available: https://directory.eoportal.org/web/eoportal/satellite-missions/s/saral. [Accessed: 26-Jul-2019].
\bibitem {} “Top 5 sources of big data | Artificial Intelligence | Data Science |,” 26-Nov-2017. [Online]. Available: https://www.allerin.com/blog/top-5-sources-of-big-data. [Accessed: 26-Jul-2019].
\bibitem {} J. Thatcher, “Big Data, Big Questions| Living on Fumes: Digital Footprints, Data Fumes, and the Limitations of Spatial Big Data,” International Journal of Communication, vol. 8, no. 0, p. 19, Jun. 2014.
\bibitem {} “Livehoods.” [Online]. Available: http://livehoods.org/research. [Accessed: 27-Jul-2019].

\end{thebibliography}
\end{document}


